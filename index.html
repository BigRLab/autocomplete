<!doctype html>
<!-- The Time Machine GitHub pages theme was designed and developed by Jon Rohan, on Feb 7, 2012. -->
<!-- Follow him for fun. http://twitter.com/jonrohan. Tail his code on http://github.com/jonrohan -->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <link rel="stylesheet" href="stylesheets/stylesheet.css" media="screen">
  <link rel="stylesheet" href="stylesheets/pygment_trac.css">
  <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
  <script type="text/javascript" src="javascripts/script.js"></script>

  <title>Autocomplete</title>
  <meta name="description" content="For you 5 yo&#39;s, I explain one way us humans can predict what somebody will say next. For us adults, how to implement [Bing,Google,Yahoo,etc.]&#39;s &quot;autocomplete&quot; feature using those so-simple-your-5-yo-can-use-it machine learning libraries out there. None.">

  <meta name="viewport" content="width=device-width,initial-scale=1">

</head>

<body>

  <div class="wrapper">
    <header>
      <h1 class="title">Autocomplete</h1>
    </header>
    <div id="container">
      <p class="tagline">For you 5 yo&#39;s, I explain one way us humans can predict what somebody will say next. For us adults, how to implement [Bing,Google,Yahoo,etc.]&#39;s &quot;autocomplete&quot; feature using those so-simple-your-5-yo-can-use-it machine learning libraries out there. None.</p>
      <div id="main" role="main">
        <div class="download-bar">
        <div class="inner">
          <a href="https://github.com/rodricios/autocomplete/tarball/master" class="download-button tar"><span>Download</span></a>
          <a href="https://github.com/rodricios/autocomplete/zipball/master" class="download-button zip"><span>Download</span></a>
          <a href="https://github.com/rodricios/autocomplete" class="code">View Autocomplete on GitHub</a>
        </div>
        <span class="blc"></span><span class="trc"></span>
        </div>
        <article class="markdown-body">
          <h2>
<a id="contents" class="anchor" href="#contents" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contents</h2>

<h3>
<a id="part-i---introduction" class="anchor" href="#part-i---introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part I - Introduction</h3>

<ul>
<li><p>Preface</p></li>
<li><p>That which doesn't exist</p></li>
<li><p><em>They, I mean *</em>it** took our jobs!*</p></li>
<li><p>Occam's Razor, the only razor to give your kids</p></li>
</ul>

<h2>
<a id="part-ii---p-a-" class="anchor" href="#part-ii---p-a-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part II - P( A )</h2>

<ul>
<li><p>The Curator Thought Experiment</p></li>
<li><p>The Domain</p></li>
<li><p>The Frequency</p></li>
<li><p>How Google, et. al. is saving you time (optional)</p></li>
<li><p>Problem [?]</p></li>
</ul>

<h2>
<a id="part-iii---p-a--b-" class="anchor" href="#part-iii---p-a--b-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part III - P( A &amp; B )</h2>

<ul>
<li><p>Probability of B given A </p></li>
<li><p>Autocomplete, suggest-as-you-type,...</p></li>
</ul>

<h3>
<a id="preface" class="anchor" href="#preface" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preface</h3>

<p>Ahh, the mysterious <em>autocomplete</em>. It has yet to receive a proper theoretical and practical explanation as is evidenced by the top results from a <a href="https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=how%20to%20implement%20autocomplete">Google search</a>. Now don't get me wrong. I consider StackOverflow to be a great resource. But as if I couldn't find a bigger brush to paint that community with, the answers one will find there are great for those with some programming/C.Sci./etc. background but not so great for those with little to no background. </p>

<p>Now some of you may ask, "So what?"</p>

<p>I cordially ask those who asked the above question to not let my commentary drive you away just yet. Please skip this and head over to my tl;dr or go straight to the beginning of what I think is the <em>actual</em> content to this article.</p>

<p>Now, if you feel like reading the thoughts of some unreliable character who hasn't even worked on some of the big kid problems at any of the rising/falling stars in the startup biosphere, read the next couple of paragraphs.</p>

<h3>
<a id="how-can-one-love-or-hate-that-which-doesnt-exist" class="anchor" href="#how-can-one-love-or-hate-that-which-doesnt-exist" aria-hidden="true"><span class="octicon octicon-link"></span></a>How can one love or hate that which doesn't exist?</h3>

<p>Artificial intelligence is a terribly phrased <em>thing</em>. I say "thing" because it isn't a <em>person</em> and it isn't a <em>place</em>. What else is <em>AI</em> not? It isn't a singular idea. I mean, you can easily bing "artificial intelligence" on Google and see for yourself - the <em>AI king or queen (whatever gender it prefers)</em> can't even give you a straight answer (compare this to searching for "machine learning," a slightly less-nebulous-sounding word that comes closer to defining the field responsible for all this <a href="http://www.independent.co.uk/news/science/stephen-hawking-right-about-dangers-of-ai-but-for-the-wrong-reasons-says-eminent-computer-expert-9908450.html">end-o'-the-world</a>, <a href="http://www.rethinkrobotics.com/artificial-intelligence-tool-threat/">great-for-the-world</a> <a href="http://www.livescience.com/49009-future-of-artificial-intelligence.html">back and forth</a> <a href="http://www.cnet.com/news/elon-musk-we-are-summoning-the-demon-with-artificial-intelligence/">hubbub</a>. </p>

<p>At best, AI is a set of loosely related ideas and concepts. By "loosely", I mean it's whatever you think it means. </p>

<p>Imagine a scale where on one end you have words that easilly allow for confusion, and on the other end you have words that allow for concensus. Now ask yourself where things like religion, <a href="http://www.forbes.com/sites/robertwood/2014/12/16/20-really-stupid-things-in-the-u-s-tax-code/">U.S. tax code</a>, Rorschach inkblots and AI would lie. Now do the same with things like the addition operator, gravity, water. Wildcards like the platypus are hard to pin down, so disregard those.  </p>

<p>Ok, so where am I going with this? What I'm getting at is that no matter where your definition of "AI" lands on that imaginary scale, it would do everyone a service if somehow we could shift the definition towards the <em>more concensus, less confusion</em> end. </p>

<p>Now how does one even begin to do this? For AI, I can't say I have an answer, but I have my intuitions. For one, force yourself to read what's being published from places not financially dependent on you reading their work (I've linked a few examples of those "financially dependent" places a couple of paragraphs back). I suggest you search up <a href="http://www.bing.com/search?q=artificial+intelligence+research">"academic papers on</a> <a href="https://www.google.com/#safe=off&amp;q=research+papers+on+artificial+intelligence">artificial intelligence"</a>. </p>

<p>Yes, some of those papers will unintentionally bore you, some of them intentionally will not. But I can almost guarantee that they aren't trying to convince you to fear or stop fearing AI. Now, as a segway towards the actual content in this article, I'll say that those papers I'm asking you to find may give you an impression that AI or machine learning (ML) is <em>all</em> a game of chance. That is to say, if it's all game of chance, by definition there's going to be winners, and the ones who win are the ones who can predict, more often than not, the <em>right answer</em>. There, enough with my opinion.</p>

<h3>
<a id="they-i-mean-it-took-our-jobs" class="anchor" href="#they-i-mean-it-took-our-jobs" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<em>They, I mean *</em>it** took our jobs!*</h3>

<p>Note: the following couple of paragraphs were originally going to be the introduction to this article. </p>

<p>I opted for the above introduction after listening to my grandmother and my third cousin (whom I'll refer to as my uncle) discuss the implications of "AI" (or "AGI"). Both of them, I believe, had valid points. </p>

<p>My grandmother worried about the inevitable influx of the unnemployed. </p>

<p>My uncle, who's around my grandmother's age, looked at the horizon a bit more optimistically and responded to my grandmother with an example of a "lonely old person" - imagine a machine that would help those older souls move around the house or the city; this machine can take over cooking, cleaning, entertaining (even the most artificial form of entertainment is <em>entertainment</em>), all amounting to, my uncle said, people <em>surviving</em> indepedently. </p>

<p>I found myself nodding to every <em>quality-of-life</em> "feature" in this hypothetical robot of the near future. </p>

<p>My grandmother slapped back with a statement so simple and prescient, it's the kind of <em>obviousness in reality</em> that gentlemen like Hitchens or Harris would have admired: "for every one of those robots, 3 to 5 caretakers will lose their job." </p>

<p>And then I jumped in and said, "where'd ya get those numbers <em>grandma</em>?" The look of utter defeat stared back at me. Surprisingly, that face also looks like the <em>you have to be kidding me"</em> face. </p>

<p>Just joking folks, I was too busy enjoying her Peruvian stew.</p>

<h3>
<a id="occams-razor-the-only-sharp-object-to-give-to-your-kids" class="anchor" href="#occams-razor-the-only-sharp-object-to-give-to-your-kids" aria-hidden="true"><span class="octicon octicon-link"></span></a>Occam's Razor, the only sharp object to give to your kids</h3>

<p>This article doesn't claim to be that elusive "proper theoretical and practical explanation" I referred to at the very beginning of this article. Instead, this is an exercise for both the reader and myself.</p>

<p>I'm going to challenge myself to explain and implement <em>autocomplete</em> while holding myself accountable under <a href="http://en.wikipedia.org/wiki/Occam's_razor">Occam's Razor</a> - a principle setforth by William of Occam (14th century philosopher) and reiterated in more relevant terms by Michael Lant in his 2010 post titled <a href="http://michaellant.com/2010/08/10/occams-razor-and-the-art-of-software-design/"><em>Occam's Razor and the Art of Software Design</em></a>. </p>

<p>And for the reader, I'm going to challenge him or her or it (for future <a href="http://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a> web crawlers) to keep things simple, to keep things at a level where intuition overpowers hard theory. </p>

<p><em>Occam's Razor</em> itself can and has been worded in a million different ways (kind of ironic*, huh?); here's a version I've come to remember:</p>

<pre><code>The simplest solution is often the best solution.
</code></pre>

<p>"Best" in what way? I don't really know. But it's a catchy phrase and if you start spitting your best catchphrases at the water cooler tomorrow, you'll likely be seen as the smart &amp; cool coworker at the office place that doesn't exist. Try it out and let me know.</p>

<h2>
<a id="part-ii---p-a--1" class="anchor" href="#part-ii---p-a--1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part II - P( A )</h2>

<h3>
<a id="the-curator-thought-experiment" class="anchor" href="#the-curator-thought-experiment" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Curator Thought Experiment</h3>

<p>Imagine you are an art curator. Your job is to care for and study the famous works of both dead and living artists from across the world. You find this to be really enjoyable. </p>

<p>And you keep on finding this enjoyable. until one day, all the world's artists vanish. Surprising or not, the museum's patrons seem to not have noticed and they continue with their occasional visits. Their apparent lack of unawareness convinces you to continue on with your work.</p>

<p>And so just like before, all carries on like clockwork, that is until people begin to express their desires for new art exhibitions. You explain to them, "There likely won't be any <em>new</em> exhibitions. The world's artists dissapeared off the face of the planet, don't you see?" </p>

<p>They look at you in bewilderment, "No they haven't, they've all moved on to discovering the magic of <em>Littlest Pet Shop</em>"</p>

<p>You respond, "You don't say..."</p>

<p>You take a moment to think about what you've just heard. You eventually think to yourself, <em>They've put down their brush. What do I do, brain? What do... Aha!</em> You arrive at this thought: it is you who must continue where others have left off. You will be the artist the world needs!</p>

<p>Pretty straight forward right? Let's put pause on the experiment, and appreciate the scope of the curator's newly defined problem. He or she must gather the collection of all unfinished works from across the world. And you will finish these unfinished works first.</p>

<p>Let's call this problem A. </p>

<p>Problem B is the next problem. That is to say the curator is also tasked with creating <em>new</em> works, but let's worry about this later. </p>

<p>Now, I'm going to be an intrusive narrator and force you, the curator, to accept that you've mentally come to a solution for problem A. In order to complete the unfinished works, you've decided to study the works of each artist to such a degree, you will in effect mimick their style. You attempt this, knowingly or not, under two constraints: domain and frequency. </p>

<h3>
<a id="the-domain" class="anchor" href="#the-domain" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Domain</h3>

<p>Allowing yourself to relate <em>domain</em> with <em>works by artist</em>, and <em>frequency</em> with <em>frequencies of motifs (or patterns)</em>, you begin solving this problem by gathering all the individual pieces in the domain (which I'll usually refer to as our <em>corpus</em>). This corpus then becomes the works of a particular artist you are attempting to emulate:</p>

<div class="highlight highlight-python"><pre>
<span class="pl-c">#pip install eatiht</span>
<span class="pl-k">import</span> eatiht

<span class="pl-c">#the entirety of the text is our "corpus"</span>
text <span class="pl-k">=</span> eatiht.extract(<span class="pl-s1"><span class="pl-pds">'</span>http://en.wikipedia.org/wiki/Shapes<span class="pl-pds">'</span></span>)
<span class="pl-k">print</span>(<span class="pl-s1"><span class="pl-pds">"</span><span class="pl-cce">\"</span><span class="pl-pds">"</span></span> <span class="pl-k">+</span> text[:<span class="pl-c1">175</span>] <span class="pl-k">+</span> <span class="pl-s1"><span class="pl-pds">"</span>...<span class="pl-cce">\"</span><span class="pl-pds">"</span></span>)
</pre></div>

<p>"""output:</p>

<p>"This article is about describing the shape of an object eg. shapes like a triangle.  For common shapes, see  list of geometric shapes .  For other uses, see  Shape (disambigua..."</p>

<p>"""</p>

<p>You neatly arrange the works so that you can quickly go from one work to the next.</p>

<div class="highlight highlight-python"><pre><span class="pl-c">#a neater representation of our "corpus"</span>
corpus <span class="pl-k">=</span> text.split()
<span class="pl-k">print</span>(corpus[:<span class="pl-c1">10</span>])

<span class="pl-c">#output</span>
[<span class="pl-s1"><span class="pl-pds">'</span>This<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>article<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>is<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>about<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>describing<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>the<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>shape<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>of<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>an<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>object<span class="pl-pds">'</span></span>]</pre></div>

<p>Now you take a look at the unnamed artist's unfinished piece: ⊏</p>

<pre><code>unfinished_word = "sq"
</code></pre>

<p>At this point, you make the following statement of fact: "He was in the process of painting a rectangular shape." To get a feel for how he produces rectangular shapes, you narrow the domain down to elements that bear some similarity.</p>

<div class="highlight highlight-python"><pre>
words_starting_with_squa <span class="pl-k">=</span> [word <span class="pl-k">for</span> word <span class="pl-k">in</span> corpus <span class="pl-k">if</span> word.startswith(unfinished_word)]
<span class="pl-k">print</span>(words_starting_with_squa)

<span class="pl-c">#output</span>
[<span class="pl-s1"><span class="pl-pds">'</span>squares<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>square<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>square<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>square<span class="pl-pds">'</span></span>, <span class="pl-s1"><span class="pl-pds">'</span>square<span class="pl-pds">'</span></span>]
</pre></div>

<p>The artist could have been trying to express a square or a rectangle or ▣ or ⊡ or ⊑ (these are all rectangular shapes, in case the unicode doesn't render). </p>

<h3>
<a id="the-frequency" class="anchor" href="#the-frequency" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Frequency</h3>

<p>Taking a <a href="http://en.wikipedia.org/wiki/Probability_interpretations#Classical_definition">probabilistic interpretation</a> of how the unfinished work should be completed, you make the assumption that whatever square shape the artist drew most <em>frequently</em> prior to giving up the art life was likely to be the shape in this work.</p>

<div class="highlight highlight-python"><pre>
<span class="pl-k">from</span> collections <span class="pl-k">import</span> Counter

<span class="pl-c">#the Counter class is a simple frequency distribution/histogram</span>
suggested <span class="pl-k">=</span> Counter([word <span class="pl-k">for</span> word <span class="pl-k">in</span> words_starting_with_squa 
                         <span class="pl-k">if</span> word.startswith(unfinished_word)]).most_common(<span class="pl-c1">10</span>)
</pre></div>

<p>At this point, be it right or wrong, you can make some guess that will <em>probably</em> contain the right answer.</p>

<p>And the suggested shapes (or words) are..?</p>

<pre><code>print(suggested)
#[('square', 4), ('squares', 1)]
</code></pre>

<p>You finished problem A. </p>

<p>One of the implications of what you've just accomplished could be this: you've created a way of typing less while still conveying your message. If you care to see a rough calculation showing you what I mean, read this next section. If not, skip it :)</p>

<h3>
<a id="how-google-et-al-is-saving-you-time" class="anchor" href="#how-google-et-al-is-saving-you-time" aria-hidden="true"><span class="octicon octicon-link"></span></a>How Google, et. al. is saving you time</h3>

<p>Question: How much <em>time</em> have search engine's <em>autocomplete</em> feature saved people?</p>

<p>To come up with a ballpark estimation, here's a possible approach: </p>

<p>Find some number of people who use search engines everyday. Let's also include a value that describes the <em>average</em> search query length (in # of words). We should also decide on what value will represent the average number of characters per word <strong>and</strong> some number of characters being saved per word. Finally, define some arbitrary constant factor that describes <em>time per key press</em>. </p>

<p>You end up with values like these:</p>

<ul>
<li><p>20.3 billion searches in one month [<a href="http://moz.com/beginners-guide-to-seo/how-people-interact-with-search-engines">source</a>]. This purports to measure only American's usage (circa October 2011) - sorry rest of the world :( </p></li>
<li><p>3 words is purported to be the maximum average [<a href="http://www.rimmkaufman.com/blog/google-instant-query-behavior-and-the-long-tail/27092012/">source</a>], some places claim that more than 50% of users use more than 3 words [<a href="http://www.smallbusinesssem.com/google-query-length/3273/">source</a>], and some claim a solid avg. of 4 words [<a href="http://www.brafton.com/news/seo-keyword-alert-long-tail-search-most-common-on-ask-com">source</a>]. Let's decide on 3.5 shall we?</p></li>
<li><p>5.1 characters according to <a href="http://www.wolframalpha.com/input/?i=average+english+word+length">Wolfram Alpha</a></p></li>
<li><p>The number of characters saved with autocompletion is currently not known (come on Google!). So let's just say it saves 1 character per word (this is probably too generous). </p></li>
<li><p>A purpotedly studied conversion factor for time-per-keypress is practically non-existant. Instead, I'll borrow one of the <a href="http://stackoverflow.com/questions/4098678/average-inter-keypress-time-when-typing">StackOverflow answers</a>: 167 ms. </p></li>
</ul>

<p>With these numbers, we can state:</p>

<p>Without autocompletion:</p>

<pre><code>20,300,000,000 searches/month * 3.5 words/search * 5.1 characters/word * 0.167 seconds/keypress 
 = 1.681 x 10e7 hours
 = 1918 years worth of human time spent searching on search engines
</code></pre>

<p>With autocompletion (saving you from having to type 1 character per word):</p>

<pre><code>1918 years - (1918 years * 4.1 / 5.1)
 = 376 years worth of human time per saved per month (back in 2011!)
</code></pre>

<p>Do not reference this <a href="http://www.wolframalpha.com/input/?i=%2820300000000+*+3.5+*+5.1+*+0.167%29+-+%2820300000000+*+3.5+*+4.1+*+0.167%29+seconds+to+hours">calculation</a> as a legitimate answer to the question. </p>

<p>Back to the implementation of autocomplete.</p>

<h3>
<a id="problem-" class="anchor" href="#problem-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem [?]</h3>

<p>Let's put the "curator" thought experiment away indefinitely (the metaphor is difficult to continue for this next part without stepping over too many toes and without having my and your brain rage quit). What do you think should replace the [?] in the above heading? </p>

<p>*Hint: count up the number of times you see "Problem" and consider also counting the occurence of the following word. Try to take a minute and see if you can imagine <em>how</em> to predict the next word given the previous word. </p>

<p>...</p>

<p>You should hopefully have some idea of what the answer is and how it is you've arrived at the answer. There's an issue I should bring up though. Your approach and my approach aren't guaranteed to lead to the same answers. In fact, there's always the possibility that our answers are incorrect.</p>

<p>If you don't happen to not feel confident with an approach to predict the next word given the previous word, that's alright! I'll walk through the steps of coming up with one of likely many solutions. </p>

<h2>
<a id="part-iii---p-a--b--1" class="anchor" href="#part-iii---p-a--b--1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part III - P( A &amp; B )</h2>

<h3>
<a id="probability-of-b-given-a" class="anchor" href="#probability-of-b-given-a" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability of B given A</h3>

<pre><code>P( A &amp; B ) = P( B | A ) * P( A )
</code></pre>

<p>Note: the "|" translates to "given". The above is what you'll find in textbooks and everywhere else where "conditional probability" is mentioned. </p>

<p>...</p>

<p>I hope that you can see how easy and potentially useful it is to use frequency distributions. We've already shown how to "predict," loosely speaking, the final form of an unfinished word by keeping count of all the words that start with our unfinished word. Sort those (word, frequency) pairs from highest to lowest and you've satisfied most people's definition of "suggestion."</p>

<p>Now widen your view to include the previous word of an unfinished word.</p>

<p>Why am I suggesting this? There's a number of reasons that are based off intuition, and they may or may not be founded on evidence, so instead of giving you my opinion, I'll refer the reader to section 1.2 of ch. 6 in the <a href="http://www.nltk.org/book/ch06.html">NLTK book</a>. There you'll find the authors have experimented with many different <em>features</em> (hand-picked elements in the corpus that may or may not turn out to hold predictive properties); some of those features ended up improving their model's ability to predict a word's part of speech. Granted, we aren't doing that, but my suggestion still stands.</p>

<p>If I were to give you what I think is a fair justification for considering  the previous word, it'd be this: something tells me that it's alright to assume that in our language (and probably many other languages) there exists pairs of words that are more likely to appear next to each other:</p>

<pre><code>a plane
</code></pre>

<p>And there are pairs of words not likely to appear next to each other:</p>

<pre><code>try automobile
</code></pre>

<p>I believe it's fair to say that it's unlikely that you've read a sentence with that exact sequence of words.</p>

<p>At this point you might think, "Ok, I get it, let's create a frequency distribution where we count up how many times a <em>pair</em> of words appear, and we'll find the pair that occurs the most. This would be wrong (I think).</p>

<p>What I'm proposing to do instead is the following: </p>

<ol>
<li><p>Create a list of pairs: <em>word, next word</em> (or <em>previous word, word</em> or <em>word A, word B</em>)</p></li>
<li>
<p>Use the first word as a key (<em>reference or anchor</em>) leading to "mini" frequency distributions across all the second word(s) that follow said first word.</p>

<ul>
<li>Another way to say this is in the form of a question: "<em>Given that the first event (or word) occurred, what is the probability that the second event (or word) will occur?</em>"</li>
</ul>
</li>
</ol>

<p>And in Python:</p>

<div class="highlight highlight-python"><pre>

<span class="pl-k">try</span>: 
    <span class="pl-s3">range</span> <span class="pl-k">=</span> <span class="pl-s3">xrange</span>
<span class="pl-k">except</span> <span class="pl-s3">NameError</span>:
    <span class="pl-k">pass</span>

<span class="pl-c">#First, "chunks" is a helper function that can partition [1,2,3,4] into [[1,2],[3,4]]</span>
<span class="pl-st">def</span> <span class="pl-en">chunks</span>(<span class="pl-vpf">l</span>, <span class="pl-vpf">n</span>):
    <span class="pl-s1"><span class="pl-pds">"""</span> Yield successive n-sized chunks from l.</span>
<span class="pl-s1">    <span class="pl-pds">"""</span></span>
    <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-s3">range</span>(<span class="pl-c1">0</span>, <span class="pl-s3">len</span>(l) <span class="pl-k">-</span> n <span class="pl-k">+</span> <span class="pl-c1">1</span>):
        <span class="pl-k">yield</span> l[i:i<span class="pl-k">+</span>n]

<span class="pl-c">#Step 1.</span>
wordpairs <span class="pl-k">=</span> <span class="pl-s3">list</span>(chunks(corpus,<span class="pl-c1">2</span>))

<span class="pl-c">#Step 2. </span>
<span class="pl-c">#Here I create a dictionary with the first word as the key, and the value will hold the "mini frequency distributions"</span>
given_prev_word_model <span class="pl-k">=</span> {wordpair[<span class="pl-c1">0</span>].lower(): Counter() <span class="pl-k">for</span> wordpair <span class="pl-k">in</span> wordpairs}

<span class="pl-c">#Here start creating our distributions where each distribution measures the </span>
<span class="pl-c"># frequency of the second word GIVEN that the first word preceded it</span>
<span class="pl-k">for</span> wordpair <span class="pl-k">in</span> wordpairs:
    <span class="pl-k">try</span>:
        <span class="pl-c">#The ".lower()" is a quick and largely inneffective way to "normalize"</span>
        given_prev_word_model[wordpair[<span class="pl-c1">0</span>].lower()].update([wordpair[<span class="pl-c1">1</span>].lower()])
    <span class="pl-c">#bandaid fix in the case that we do not have an even # of words in our corpus</span>
    <span class="pl-k">except</span>: 
        <span class="pl-k">pass</span>    
</pre></div>

<p>I need to note that the process of building a predictive model from some reference data (corpus, domain, etc.) is often called "training", as in: <em>to train your 'autocomplete' prediction model.</em></p>

<p>Now to see the prediction of a word given the previous word, come up with a  string with two words where the second word is partially typed:</p>

<h1>
<a id="i-should-make-it-clear-that-in-this-hypothetical-case" class="anchor" href="#i-should-make-it-clear-that-in-this-hypothetical-case" aria-hidden="true"><span class="octicon octicon-link"></span></a>I should make it clear that in this hypothetical case,</h1>

<h1>
<a id="you-are-in-the-middle-of-typing-your-second-word" class="anchor" href="#you-are-in-the-middle-of-typing-your-second-word" aria-hidden="true"><span class="octicon octicon-link"></span></a>you are in the middle of typing your second word.</h1>

<p>text = "The s"</p>

<h3>
<a id="finally-autocomplete-suggest-as-you-type" class="anchor" href="#finally-autocomplete-suggest-as-you-type" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finally, autocomplete, suggest-as-you-type,...</h3>

<p>The next function can be seen as a helper function, but in the realm of Statistics and Probability, it's what's commonly referred to as the "ARGMAX" query, or in a less cryptic fashion: a function that returns the <em>argument</em> (or <em>word</em> in our case) that produces the <em>maximum</em> value in a given frequency distribution. </p>

<p>In other words, it's the <em>most</em> probable word to appear next, GIVEN the previous word. </p>

<div class="highlight highlight-python"><pre>
<span class="pl-st">def</span> <span class="pl-en">predict_next_word</span>(<span class="pl-vpf">word_1</span>, <span class="pl-vpf">word_2</span>):
    <span class="pl-k">return</span> Counter( { w:c <span class="pl-k">for</span> w, c <span class="pl-k">in</span> given_prev_word_model[word_1.lower()].items()
                    <span class="pl-k">if</span> w.startswith(word_2.lower())} ).most_common(<span class="pl-c1">10</span>)

<span class="pl-c">#Let's use this puppy!</span>
suggestions <span class="pl-k">=</span> predict_next_word(<span class="pl-k">*</span>(text.split()))

<span class="pl-k">print</span>(suggestions)
<span class="pl-c">#output</span>
<span class="pl-c">#[('same', 15), ('shape', 11), ('subsets', 1), ('same.', 1), ('spatial', 1), ('size', 1)]</span></pre></div>

<p>At this point, we've finished. </p>

<p>Take a look at what we've been able to accomplish. We've been able to create a tool that provided you give the program the last or last two words that you've typed, it will suggest to you some words that may have been what you're in the process of typing.</p>

<p>In a <em>big picture</em> kind of way, we've created a piece to a bigger puzzle; a puzzle piece that "big" companies have thought is worthy enough to tackle. But what I think is more important here is the fact that we've been able to walk through the steps of creating a tool that's both useful and easy to explain and understand. </p>

<p>Now obviously, it's my opinion that <strong>I think</strong> I've been able to present this in a manner that . And so I'd like to finish with one more challenge to the reader</p>

<h3>
<a id="todo" class="anchor" href="#todo" aria-hidden="true"><span class="octicon octicon-link"></span></a>TODO!</h3>

<h3>
<a id="afterword" class="anchor" href="#afterword" aria-hidden="true"><span class="octicon octicon-link"></span></a>Afterword</h3>

<p>notes: *I have to give a shout out to <a href="https://twitter.com/SamHarrisOrg">Sam Harris</a> for wonderfully <a href="https://www.youtube.com/watch?v=pCofmZlC72g#t=1144">putting into words</a> what I've borrowed and slightly adapted for this writing. </p>

<p>Another shoutout to <a href="http://norvig.com">Peter Norvig</a> for inspiring me and probably many others with what many will likely consider to be an almost full on copy paste of his <a href="http://norvig.com/spell-correct.html"><em>How to Write a Spell Checker</em></a>(! But I swear it's not! I actually I think I may have out-Norvig'ed Peter Norvig when it comes to describing <a href="http://en.wikipedia.org/wiki/Conditional_probability">conditional probability</a>: P(wordA &amp; wordB) = P(wordB | wordA)*P(wordA)</p>

<p>And another one to Rob Renaud's <a href="https://github.com/rrenaud/Gibberish-Detector">Gibberish Detector</a>. I, out of pure chance, ran into his project some time after running into Norvig's article. I can't describe <em>how</em> much it helped solidify what the heavy hitters of "AI" consider to be introductory material. </p>

<p>I can't describe it because I literally <em>can't</em> describe it, that is, without me using "likely" and "probably" in what would have likely been every sentence, but who really knows? </p>

<p>Warning! Second article about this exact thing, only expressed differently, coming soon! Oh and the code too, that is if someone hasn't gotten to translating the above article to code before I can get to uploading the project :P I'm trying to get the kinks out of here and the code so it's simple, duh!</p>
        </article>
      </div>
    </div>
    <footer>
      <div class="owner">
      <p><a href="https://github.com/rodricios" class="avatar"><img src="https://avatars3.githubusercontent.com/u/6603965?v=3&amp;s=60" width="48" height="48"></a> <a href="https://github.com/rodricios">rodricios</a> maintains <a href="https://github.com/rodricios/autocomplete">Autocomplete</a></p>


      </div>
      <div class="creds">
        <small>This page generated using <a href="http://pages.github.com/">GitHub Pages</a><br>theme by <a href="https://twitter.com/jonrohan/">Jon Rohan</a></small>
      </div>
    </footer>
  </div>
  <div class="current-section">
    <a href="#top">Scroll to top</a>
    <a href="https://github.com/rodricios/autocomplete/tarball/master" class="tar">tar</a><a href="https://github.com/rodricios/autocomplete/zipball/master" class="zip">zip</a><a href="" class="code">source code</a>
    <p class="name"></p>
  </div>

            <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-59945997-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

</body>
</html>
