{"name":"Autocomplete","tagline":"For you 5 yo's, I explain one way us humans can predict what somebody will say next. For us adults, how to implement [Bing,Google,Yahoo,etc.]'s \"autocomplete\" feature using those so-simple-your-5-yo-can-use-it machine learning libraries out there. None.","body":"\r\n##Contents\r\n\r\n### Part I - Introduction\r\n\r\n* Preface\r\n\r\n* That which doesn't exist\r\n\r\n* *They, I mean **it** took our jobs!*\r\n\r\n* Occam's Razor, the only razor to give your kids\r\n\r\n## Part II - P( A )\r\n\r\n* The Curator Thought Experiment\r\n \r\n* The Domain\r\n\r\n* The Frequency\r\n\r\n* How Google, et. al. is saving you time (optional)\r\n\r\n* Problem [?]\r\n\r\n## Part III - P( A & B ) \r\n\r\n* Probability of B given A \r\n\r\n* Autocomplete, suggest-as-you-type,...\r\n\r\n### Preface\r\n\r\nAhh, the mysterious *autocomplete*. It has yet to receive a proper theoretical and practical explanation as is evidenced by the top results from a [Google search](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=how%20to%20implement%20autocomplete). Now don't get me wrong. I consider StackOverflow to be a great resource. But as if I couldn't find a bigger brush to paint that community with, the answers one will find there are great for those with some programming/C.Sci./etc. background but not so great for those with little to no background. \r\n\r\nNow some of you may ask, \"So what?\"\r\n\r\nI cordially ask those who asked the above question to not let my commentary drive you away just yet. Please skip this and head over to my tl;dr or go straight to the beginning of what I think is the *actual* content to this article.\r\n\r\nNow, if you feel like reading the thoughts of some unreliable character who hasn't even worked on some of the big kid problems at any of the rising/falling stars in the startup biosphere, read the next couple of paragraphs.\r\n\r\n###How can one love or hate that which doesn't exist?\r\n\r\nArtificial intelligence is a terribly phrased *thing*. I say \"thing\" because it isn't a *person* and it isn't a *place*. What else is *AI* not? It isn't a singular idea. I mean, you can easily bing \"artificial intelligence\" on Google and see for yourself - the *AI king or queen (whatever gender it prefers)* can't even give you a straight answer (compare this to searching for \"machine learning,\" a slightly less-nebulous-sounding word that comes closer to defining the field responsible for all this [end-o'-the-world](http://www.independent.co.uk/news/science/stephen-hawking-right-about-dangers-of-ai-but-for-the-wrong-reasons-says-eminent-computer-expert-9908450.html), [great-for-the-world](http://www.rethinkrobotics.com/artificial-intelligence-tool-threat/) [back and forth](http://www.livescience.com/49009-future-of-artificial-intelligence.html) [hubbub](http://www.cnet.com/news/elon-musk-we-are-summoning-the-demon-with-artificial-intelligence/). \r\n\r\nAt best, AI is a set of loosely related ideas and concepts. By \"loosely\", I mean it's whatever you think it means. \r\n\r\nImagine a scale where on one end you have words that easilly allow for confusion, and on the other end you have words that allow for concensus. Now ask yourself where things like religion, [U.S. tax code](http://www.forbes.com/sites/robertwood/2014/12/16/20-really-stupid-things-in-the-u-s-tax-code/), Rorschach inkblots and AI would lie. Now do the same with things like the addition operator, gravity, water. Wildcards like the platypus are hard to pin down, so disregard those.  \r\n\r\nOk, so where am I going with this? What I'm getting at is that no matter where your definition of \"AI\" lands on that imaginary scale, it would do everyone a service if somehow we could shift the definition towards the *more concensus, less confusion* end. \r\n\r\nNow how does one even begin to do this? For AI, I can't say I have an answer, but I have my intuitions. For one, force yourself to read what's being published from places not financially dependent on you reading their work (I've linked a few examples of those \"financially dependent\" places a couple of paragraphs back). I suggest you search up [\"academic papers on](http://www.bing.com/search?q=artificial+intelligence+research) [artificial intelligence\"](https://www.google.com/#safe=off&q=research+papers+on+artificial+intelligence). \r\n\r\nYes, some of those papers will unintentionally bore you, some of them intentionally will not. But I can almost guarantee that they aren't trying to convince you to fear or stop fearing AI. Now, as a segway towards the actual content in this article, I'll say that those papers I'm asking you to find may give you an impression that AI or machine learning (ML) is *all* a game of chance. That is to say, if it's all game of chance, by definition there's going to be winners, and the ones who win are the ones who can predict, more often than not, the *right answer*. There, enough with my opinion.\r\n\r\n### *They, I mean **it** took our jobs!*\r\n\r\nNote: the following couple of paragraphs were originally going to be the introduction to this article. \r\n\r\nI opted for the above introduction after listening to my grandmother and my third cousin (whom I'll refer to as my uncle) discuss the implications of \"AI\" (or \"AGI\"). Both of them, I believe, had valid points. \r\n\r\nMy grandmother worried about the inevitable influx of the unnemployed. \r\n\r\nMy uncle, who's around my grandmother's age, looked at the horizon a bit more optimistically and responded to my grandmother with an example of a \"lonely old person\" - imagine a machine that would help those older souls move around the house or the city; this machine can take over cooking, cleaning, entertaining (even the most artificial form of entertainment is *entertainment*), all amounting to, my uncle said, people *surviving* indepedently. \r\n\r\nI found myself nodding to every *quality-of-life* \"feature\" in this hypothetical robot of the near future. \r\n\r\nMy grandmother slapped back with a statement so simple and prescient, it's the kind of *obviousness in reality* that gentlemen like Hitchens or Harris would have admired: \"for every one of those robots, 3 to 5 caretakers will lose their job.\" \r\n\r\nAnd then I jumped in and said, \"where'd ya get those numbers *grandma*?\" The look of utter defeat stared back at me. Surprisingly, that face also looks like the *you have to be kidding me\"* face. \r\n\r\nJust joking folks, I was too busy enjoying her Peruvian stew.\r\n\r\n### Occam's Razor, the only sharp object to give to your kids\r\n\r\nThis article doesn't claim to be that elusive \"proper theoretical and practical explanation\" I referred to at the very beginning of this article. Instead, this is an exercise for both the reader and myself.\r\n\r\nI'm going to challenge myself to explain and implement *autocomplete* while holding myself accountable under [Occam's Razor](http://en.wikipedia.org/wiki/Occam's_razor) - a principle setforth by William of Occam (14th century philosopher) and reiterated in more relevant terms by Michael Lant in his 2010 post titled [*Occam's Razor and the Art of Software Design*](http://michaellant.com/2010/08/10/occams-razor-and-the-art-of-software-design/). \r\n\r\nAnd for the reader, I'm going to challenge him or her or it (for future [AGI](http://en.wikipedia.org/wiki/Artificial_general_intelligence) web crawlers) to keep things simple, to keep things at a level where intuition overpowers hard theory. \r\n\r\n*Occam's Razor* itself can and has been worded in a million different ways (kind of ironic\\*, huh?); here's a version I've come to remember:\r\n\r\n    The simplest solution is often the best solution.\r\n    \r\n\"Best\" in what way? I don't really know. But it's a catchy phrase and if you start spitting your best catchphrases at the water cooler tomorrow, you'll likely be seen as the smart & cool coworker at the office place that doesn't exist. Try it out and let me know.\r\n\r\n## Part II - P( A )\r\n\r\n### The Curator Thought Experiment\r\n\r\nImagine you are an art curator. Your job is to care for and study the famous works of both dead and living artists from across the world. You find this to be really enjoyable. \r\n\r\nAnd you keep on finding this enjoyable. until one day, all the world's artists vanish. Surprising or not, the museum's patrons seem to not have noticed and they continue with their occasional visits. Their apparent lack of unawareness convinces you to continue on with your work.\r\n\r\nAnd so just like before, all carries on like clockwork, that is until people begin to express their desires for new art exhibitions. You explain to them, \"There likely won't be any *new* exhibitions. The world's artists dissapeared off the face of the planet, don't you see?\" \r\n\r\nThey look at you in bewilderment, \"No they haven't, they've all moved on to discovering the magic of *Littlest Pet Shop*\"\r\n\r\nYou respond, \"You don't say...\"\r\n\r\nYou take a moment to think about what you've just heard. You eventually think to yourself, *They've put down their brush. What do I do, brain? What do... Aha!* You arrive at this thought: it is you who must continue where others have left off. You will be the artist the world needs!\r\n\r\nPretty straight forward right? Let's put pause on the experiment, and appreciate the scope of the curator's newly defined problem. He or she must gather the collection of all unfinished works from across the world. And you will finish these unfinished works first.\r\n\r\nLet's call this problem A. \r\n\r\nProblem B is the next problem. That is to say the curator is also tasked with creating *new* works, but let's worry about this later. \r\n\r\nNow, I'm going to be an intrusive narrator and force you, the curator, to accept that you've mentally come to a solution for problem A. In order to complete the unfinished works, you've decided to study the works of each artist to such a degree, you will in effect mimick their style. You attempt this, knowingly or not, under two constraints: domain and frequency. \r\n\r\n###The Domain\r\n\r\nAllowing yourself to relate *domain* with *works by artist*, and *frequency* with *frequencies of motifs (or patterns)*, you begin solving this problem by gathering all the individual pieces in the domain (which I'll usually refer to as our *corpus*). This corpus then becomes the works of a particular artist you are attempting to emulate:\r\n\r\n```python\r\n\r\n#pip install eatiht\r\nimport eatiht\r\n\r\n#the entirety of the text is our \"corpus\"\r\ntext = eatiht.extract('http://en.wikipedia.org/wiki/Shapes')\r\nprint(\"\\\"\" + text[:175] + \"...\\\"\")\r\n\r\n```\r\n\r\n\"\"\"output:\r\n\r\n\"This article is about describing the shape of an object eg. shapes like a triangle.  For common shapes, see  list of geometric shapes .  For other uses, see  Shape (disambigua...\"\r\n\r\n\"\"\"\r\n\r\nYou neatly arrange the works so that you can quickly go from one work to the next.\r\n\r\n```python\r\n#a neater representation of our \"corpus\"\r\ncorpus = text.split()\r\nprint(corpus[:10])\r\n\r\n#output\r\n['This', 'article', 'is', 'about', 'describing', 'the', 'shape', 'of', 'an', 'object']\r\n```\r\n\r\nNow you take a look at the unnamed artist's unfinished piece: ⊏\r\n\r\n    unfinished_word = \"sq\"\r\n\r\nAt this point, you make the following statement of fact: \"He was in the process of painting a rectangular shape.\" To get a feel for how he produces rectangular shapes, you narrow the domain down to elements that bear some similarity.\r\n\r\n```python\r\n\r\nwords_starting_with_squa = [word for word in corpus if word.startswith(unfinished_word)]\r\nprint(words_starting_with_squa)\r\n\r\n#output\r\n['squares', 'square', 'square', 'square', 'square']\r\n\r\n```\r\n\r\n\r\nThe artist could have been trying to express a square or a rectangle or ▣ or ⊡ or ⊑ (these are all rectangular shapes, in case the unicode doesn't render). \r\n\r\n### The Frequency\r\n\r\nTaking a [probabilistic interpretation](http://en.wikipedia.org/wiki/Probability_interpretations#Classical_definition) of how the unfinished work should be completed, you make the assumption that whatever square shape the artist drew most *frequently* prior to giving up the art life was likely to be the shape in this work.\r\n\r\n```python\r\n\r\nfrom collections import Counter\r\n\r\n#the Counter class is a simple frequency distribution/histogram\r\nsuggested = Counter([word for word in words_starting_with_squa \r\n                         if word.startswith(unfinished_word)]).most_common(10)\r\n\r\n```\r\n\r\nAt this point, be it right or wrong, you can make some guess that will *probably* contain the right answer.\r\n\r\nAnd the suggested shapes (or words) are..?\r\n\r\n    print(suggested)\r\n    #[('square', 4), ('squares', 1)]\r\n\r\n\r\nYou finished problem A. \r\n\r\nOne of the implications of what you've just accomplished could be this: you've created a way of typing less while still conveying your message. If you care to see a rough calculation showing you what I mean, read this next section. If not, skip it :)\r\n\r\n### How Google, et. al. is saving you time\r\n\r\nQuestion: How much *time* have search engine's *autocomplete* feature saved people?\r\n\r\nTo come up with a ballpark estimation, here's a possible approach: \r\n\r\nFind some number of people who use search engines everyday. Let's also include a value that describes the *average* search query length (in # of words). We should also decide on what value will represent the average number of characters per word **and** some number of characters being saved per word. Finally, define some arbitrary constant factor that describes *time per key press*. \r\n\r\nYou end up with values like these:\r\n\r\n* 20.3 billion searches in one month [[source](http://moz.com/beginners-guide-to-seo/how-people-interact-with-search-engines)]. This purports to measure only American's usage (circa October 2011) - sorry rest of the world :( \r\n\r\n* 3 words is purported to be the maximum average [[source](http://www.rimmkaufman.com/blog/google-instant-query-behavior-and-the-long-tail/27092012/)], some places claim that more than 50% of users use more than 3 words [[source](http://www.smallbusinesssem.com/google-query-length/3273/)], and some claim a solid avg. of 4 words [[source](http://www.brafton.com/news/seo-keyword-alert-long-tail-search-most-common-on-ask-com)]. Let's decide on 3.5 shall we?\r\n\r\n* 5.1 characters according to [Wolfram Alpha](http://www.wolframalpha.com/input/?i=average+english+word+length)\r\n\r\n* The number of characters saved with autocompletion is currently not known (come on Google!). So let's just say it saves 1 character per word (this is probably too generous). \r\n\r\n* A purpotedly studied conversion factor for time-per-keypress is practically non-existant. Instead, I'll borrow one of the [StackOverflow answers](http://stackoverflow.com/questions/4098678/average-inter-keypress-time-when-typing): 167 ms. \r\n\r\nWith these numbers, we can state:\r\n\r\nWithout autocompletion:\r\n\r\n    20,300,000,000 searches/month * 3.5 words/search * 5.1 characters/word * 0.167 seconds/keypress \r\n     = 1.681 x 10e7 hours\r\n     = 1918 years worth of human time spent searching on search engines\r\n\r\nWith autocompletion (saving you from having to type 1 character per word):\r\n\r\n    1918 years - (1918 years * 4.1 / 5.1)\r\n     = 376 years worth of human time per saved per month (back in 2011!)\r\n\r\nDo not reference this [calculation](http://www.wolframalpha.com/input/?i=%2820300000000+*+3.5+*+5.1+*+0.167%29+-+%2820300000000+*+3.5+*+4.1+*+0.167%29+seconds+to+hours) as a legitimate answer to the question. \r\n\r\nBack to the implementation of autocomplete.\r\n\r\n### Problem [?]\r\n\r\nLet's put the \"curator\" thought experiment away indefinitely (the metaphor is difficult to continue for this next part without stepping over too many toes and without having my and your brain rage quit). What do you think should replace the [?] in the above heading? \r\n\r\n\\*Hint: count up the number of times you see \"Problem\" and consider also counting the occurence of the following word. Try to take a minute and see if you can imagine *how* to predict the next word given the previous word. \r\n\r\n...\r\n\r\nYou should hopefully have some idea of what the answer is and how it is you've arrived at the answer. There's an issue I should bring up though. Your approach and my approach aren't guaranteed to lead to the same answers. In fact, there's always the possibility that our answers are incorrect.\r\n\r\nIf you don't happen to not feel confident with an approach to predict the next word given the previous word, that's alright! I'll walk through the steps of coming up with one of likely many solutions. \r\n\r\n## Part III - P( A & B )\r\n\r\n### Probability of B given A\r\n\r\n    P( A & B ) = P( B | A ) * P( A )\r\n\r\nNote: the \"|\" translates to \"given\". The above is what you'll find in textbooks and everywhere else where \"conditional probability\" is mentioned. \r\n\r\n...\r\n\r\nI hope that you can see how easy and potentially useful it is to use frequency distributions. We've already shown how to \"predict,\" loosely speaking, the final form of an unfinished word by keeping count of all the words that start with our unfinished word. Sort those (word, frequency) pairs from highest to lowest and you've satisfied most people's definition of \"suggestion.\"\r\n\r\nNow widen your view to include the previous word of an unfinished word.\r\n\r\nWhy am I suggesting this? There's a number of reasons that are based off intuition, and they may or may not be founded on evidence, so instead of giving you my opinion, I'll refer the reader to section 1.2 of ch. 6 in the [NLTK book](http://www.nltk.org/book/ch06.html). There you'll find the authors have experimented with many different *features* (hand-picked elements in the corpus that may or may not turn out to hold predictive properties); some of those features ended up improving their model's ability to predict a word's part of speech. Granted, we aren't doing that, but my suggestion still stands.\r\n\r\nIf I were to give you what I think is a fair justification for considering  the previous word, it'd be this: something tells me that it's alright to assume that in our language (and probably many other languages) there exists pairs of words that are more likely to appear next to each other:\r\n\r\n    a plane\r\n\r\nAnd there are pairs of words not likely to appear next to each other:\r\n\r\n    try automobile\r\n\r\nI believe it's fair to say that it's unlikely that you've read a sentence with that exact sequence of words.\r\n\r\nAt this point you might think, \"Ok, I get it, let's create a frequency distribution where we count up how many times a *pair* of words appear, and we'll find the pair that occurs the most. This would be wrong (I think).\r\n\r\nWhat I'm proposing to do instead is the following: \r\n\r\n1. Create a list of pairs: *word, next word* (or *previous word, word* or *word A, word B*)\r\n\r\n2. Use the first word as a key (*reference or anchor*) leading to \"mini\" frequency distributions across all the second word(s) that follow said first word.\r\n  * Another way to say this is in the form of a question: \"*Given that the first event (or word) occurred, what is the probability that the second event (or word) will occur?*\"\r\n  \r\nAnd in Python:\r\n\r\n```python \r\n\r\n\r\ntry: \r\n    range = xrange\r\nexcept NameError:\r\n    pass\r\n\r\n#First, \"chunks\" is a helper function that can partition [1,2,3,4] into [[1,2],[3,4]]\r\ndef chunks(l, n):\r\n    \"\"\" Yield successive n-sized chunks from l.\r\n    \"\"\"\r\n    for i in range(0, len(l) - n + 1):\r\n        yield l[i:i+n]\r\n\r\n#Step 1.\r\nwordpairs = list(chunks(corpus,2))\r\n\r\n#Step 2. \r\n#Here I create a dictionary with the first word as the key, and the value will hold the \"mini frequency distributions\"\r\ngiven_prev_word_model = {wordpair[0].lower(): Counter() for wordpair in wordpairs}\r\n\r\n#Here start creating our distributions where each distribution measures the \r\n# frequency of the second word GIVEN that the first word preceded it\r\nfor wordpair in wordpairs:\r\n    try:\r\n        #The \".lower()\" is a quick and largely inneffective way to \"normalize\"\r\n        given_prev_word_model[wordpair[0].lower()].update([wordpair[1].lower()])\r\n    #bandaid fix in the case that we do not have an even # of words in our corpus\r\n    except: \r\n        pass    \r\n    \r\n```\r\n\r\n\r\nI need to note that the process of building a predictive model from some reference data (corpus, domain, etc.) is often called \"training\", as in: *to train your 'autocomplete' prediction model.*\r\n\r\nNow to see the prediction of a word given the previous word, come up with a  string with two words where the second word is partially typed:\r\n\r\n\r\n#I should make it clear that in this hypothetical case, \r\n# you are in the middle of typing your second word.\r\ntext = \"The s\"\r\n\r\n\r\n\r\n### Finally, autocomplete, suggest-as-you-type,...\r\n\r\nThe next function can be seen as a helper function, but in the realm of Statistics and Probability, it's what's commonly referred to as the \"ARGMAX\" query, or in a less cryptic fashion: a function that returns the *argument* (or *word* in our case) that produces the *maximum* value in a given frequency distribution. \r\n\r\nIn other words, it's the *most* probable word to appear next, GIVEN the previous word. \r\n\r\n```python\r\n\r\ndef predict_next_word(word_1, word_2):\r\n    return Counter( { w:c for w, c in given_prev_word_model[word_1.lower()].items()\r\n                    if w.startswith(word_2.lower())} ).most_common(10)\r\n\r\n#Let's use this puppy!\r\nsuggestions = predict_next_word(*(text.split()))\r\n                              \r\nprint(suggestions)\r\n#output\r\n#[('same', 15), ('shape', 11), ('subsets', 1), ('same.', 1), ('spatial', 1), ('size', 1)]\r\n```\r\n\r\nAt this point, we've finished. \r\n\r\nTake a look at what we've been able to accomplish. We've been able to create a tool that provided you give the program the last or last two words that you've typed, it will suggest to you some words that may have been what you're in the process of typing.\r\n\r\nIn a *big picture* kind of way, we've created a piece to a bigger puzzle; a puzzle piece that \"big\" companies have thought is worthy enough to tackle. But what I think is more important here is the fact that we've been able to walk through the steps of creating a tool that's both useful and easy to explain and understand. \r\n\r\nNow obviously, it's my opinion that **I think** I've been able to present this in a manner that . And so I'd like to finish with one more challenge to the reader\r\n\r\n###TODO!\r\n###Afterword\r\n\r\nnotes: \\*I have to give a shout out to [Sam Harris](https://twitter.com/SamHarrisOrg) for wonderfully [putting into words](https://www.youtube.com/watch?v=pCofmZlC72g#t=1144) what I've borrowed and slightly adapted for this writing. \r\n\r\nAnother shoutout to [Peter Norvig](http://norvig.com) for inspiring me and probably many others with what many will likely consider to be an almost full on copy paste of his [*How to Write a Spell Checker*](http://norvig.com/spell-correct.html)(! But I swear it's not! I actually I think I may have out-Norvig'ed Peter Norvig when it comes to describing [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability): P(wordA & wordB) = P(wordB | wordA)\\*P(wordA)\r\n\r\nAnd another one to Rob Renaud's [Gibberish Detector](https://github.com/rrenaud/Gibberish-Detector). I, out of pure chance, ran into his project some time after running into Norvig's article. I can't describe *how* much it helped solidify what the heavy hitters of \"AI\" consider to be introductory material. \r\n\r\nI can't describe it because I literally *can't* describe it, that is, without me using \"likely\" and \"probably\" in what would have likely been every sentence, but who really knows? \r\n\r\nWarning! Second article about this exact thing, only expressed differently, coming soon! Oh and the code too, that is if someone hasn't gotten to translating the above article to code before I can get to uploading the project :P I'm trying to get the kinks out of here and the code so it's simple, duh!\r\n","google":"UA-59945997-1","note":"Don't delete this file! It's used internally to help with page regeneration."}